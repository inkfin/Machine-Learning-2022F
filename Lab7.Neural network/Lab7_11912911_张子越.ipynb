{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB7 Assignment\n",
    "> 11912911 张子越\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB Assignment\n",
    "### Exercise 1 logistic regression (20 points )\n",
    "This exercise uses dataset digit01.csv , which has 13 columns, and the last column is the dependent variable. \n",
    "\n",
    "This part requires you to implement a `logistic regression` using the pytorch framework (defining a logistic regression class that inherits `nn.module`). To test your model, we provide a dataset `digit01.csv` which is in the **datasets folder**. This dataset requires you to divide the training set and the test set by yourself, and it is recommended that 80% of the training set and 20% of the test set be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   10  11  12\n",
       "0   1   1   1   1   0   1   1   0   1   1   1   1   0\n",
       "1   0   1   1   1   0   1   1   0   1   1   1   1   0\n",
       "2   1   1   0   1   0   1   1   0   1   1   1   1   0\n",
       "3   1   1   1   1   0   1   1   0   1   1   1   0   0\n",
       "4   1   1   1   1   0   1   1   0   1   0   1   1   0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "df = pd.read_csv(\"./datasets/digit01.csv\", header=None)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    32\n",
      "1    32\n",
      "Name: 12, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((64, 12), (64,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# col 13 is y\n",
    "print(df[12].value_counts())\n",
    "y = df[12]\n",
    "X = df.drop(12, axis=1)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Splitting dataset into 80% Training and 20% Testing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51, 12), (51,), (13, 12), (13,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=123)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change type to tensor\n",
    "import torch \n",
    "\n",
    "X_train = torch.from_numpy(X_train.values).float()\n",
    "y_train = torch.from_numpy(y_train.values).float().unsqueeze(1)\n",
    "X_test  = torch.from_numpy(X_test .values).float()\n",
    "y_test  = torch.from_numpy(y_test .values).float().unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Define a LogisticRegression subclass of nn. Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a LogisticRegression subclass of nn.Module.\n",
    "########### Write Your Code Here ###########\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as f\n",
    "\n",
    "class MyLogisticRegression(nn.Module):\n",
    "    def __init__(self, in_features, out_features) -> None:\n",
    "        super(MyLogisticRegression, self).__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        assert(in_features > 0 and out_features > 0)\n",
    "\n",
    "        self.device = torch.device('cpu')\n",
    "        self.dtype = torch.float\n",
    "\n",
    "        self.flattern = nn.Flatten()\n",
    "        self.linear_sigmoid_stack = nn.Sequential(\n",
    "            nn.Linear(self.in_features, self.out_features, device=self.device, dtype=self.dtype),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flattern(x)\n",
    "        y_pred = self.linear_sigmoid_stack(x)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "m, n = X_train.shape[1], 1\n",
    "model = MyLogisticRegression(m, n)\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " + Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "# criterions = torch.nn.MSELoss(reduction='sum')\n",
    "criterions = torch.nn.BCELoss(size_average=True)\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "# optimizer = torch.optim.RMSprop(params=model.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.001)\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100, loss = 0.7086\n",
      "epoch: 200, loss = 0.6852\n",
      "epoch: 300, loss = 0.6631\n",
      "epoch: 400, loss = 0.6421\n",
      "epoch: 500, loss = 0.6223\n",
      "epoch: 600, loss = 0.6034\n",
      "epoch: 700, loss = 0.5856\n",
      "epoch: 800, loss = 0.5686\n",
      "epoch: 900, loss = 0.5524\n",
      "epoch: 1000, loss = 0.5370\n",
      "epoch: 1100, loss = 0.5224\n",
      "epoch: 1200, loss = 0.5084\n",
      "epoch: 1300, loss = 0.4951\n",
      "epoch: 1400, loss = 0.4824\n",
      "epoch: 1500, loss = 0.4703\n",
      "epoch: 1600, loss = 0.4587\n",
      "epoch: 1700, loss = 0.4476\n",
      "epoch: 1800, loss = 0.4370\n",
      "epoch: 1900, loss = 0.4269\n",
      "epoch: 2000, loss = 0.4172\n"
     ]
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "for epoch in range(2000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # Forward\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterions(y_pred, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "+ Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65100086 0.25052527 0.45733142 0.582207   0.2786178  0.64463115\n",
      " 0.5874424  0.627756   0.27040705 0.16217469 0.2509038  0.22235188\n",
      " 0.5703614 ]\n",
      "[1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1.]\n",
      "[1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1.]\n",
      "accuracy: 0.9231\n"
     ]
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    y_pred = y_pred.squeeze().numpy()\n",
    "    print(y_pred)\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] > 0.5:\n",
    "            y_pred[i] = 1\n",
    "        else:\n",
    "            y_pred[i] = 0\n",
    "    print(y_pred)\n",
    "    y_test_numpy = y_test.squeeze().numpy()\n",
    "    print(y_test_numpy)\n",
    "    acc = len(y_pred[y_pred == y_test_numpy]) / float(X_test.shape[0])\n",
    "    print(f'accuracy: {acc:.4f}')\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      1.00      0.92         6\n",
      "         1.0       1.00      0.86      0.92         7\n",
      "\n",
      "    accuracy                           0.92        13\n",
      "   macro avg       0.93      0.93      0.92        13\n",
      "weighted avg       0.93      0.92      0.92        13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_numpy, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2  Handwriting recognition with MLP\n",
    "\n",
    "Like last week's lab , your task in this section is also about recognizing handwritten digits, but you are required to use MLP to complete the exercise. It is recommended that you define an MLP class, which is a subclass of `nn.module`.\n",
    "\n",
    "<font color='red' size=4>Note that your accuracy in this section will directly determine your score.</font>\n",
    "\n",
    "For this exercise we use the `minist` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28]) torch.Size([60000]) torch.Size([10000, 28, 28]) torch.Size([10000])\n",
      "torch.float32 torch.int64\n",
      "torch.Size([60000, 784]) tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([5, 0, 4, 1, 9]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "train_ds = datasets.MNIST('./datasets', train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "test_ds = datasets.MNIST('./datasets/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "print(train_ds.data.shape, train_ds.targets.shape, test_ds.data.shape, test_ds.targets.shape)\n",
    "\n",
    "X_train = train_ds.data.float().flatten(1)\n",
    "y_train = train_ds.targets.long()\n",
    "X_test  = test_ds .data.float().flatten(1)\n",
    "y_test  = test_ds .targets.long()\n",
    "\n",
    "print(X_train.dtype, y_train.dtype)\n",
    "\n",
    "# X_train_ = torch.flatten(X_train, 1)\n",
    "\n",
    "print(X_train.shape, y_train.unique())\n",
    "\n",
    "X_train[:5], y_train[:5]\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(78.5675)\n",
      "tensor(27525.)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import normalize\n",
    "print(X_train.std())\n",
    "print(X_train[0].sum())\n",
    "\n",
    "# X_train_norm = normalize(X_train)\n",
    "# print(X_train_norm.std())\n",
    "# print(X_train_norm[0].sum())\n",
    "\n",
    "# X_train = X_train_norm\n",
    "# X_test = normalize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Define a MLP subclass of nn. Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "class MyMLP(nn.Module):\n",
    "    def __init__(self, ni, h1, h2, no) -> None:\n",
    "        super(MyMLP, self).__init__()\n",
    "        self.MLP_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(ni, h1),\n",
    "            # nn.BatchNorm1d(h1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            # nn.Linear(h1, h2),\n",
    "            # # nn.BatchNorm1d(h2),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(0.5),\n",
    "            nn.Linear(h1, no)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.MLP_stack(x)\n",
    "        return logits\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "i, h1, h2, o = 784, 512, 128, 10\n",
    "\n",
    "model = MyMLP(i, 392, h2, o)\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " + Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100, loss = 1.1071\n",
      "epoch: 200, loss = 0.7333\n",
      "epoch: 300, loss = 0.5919\n",
      "epoch: 400, loss = 0.5089\n",
      "epoch: 500, loss = 0.4586\n",
      "epoch: 600, loss = 0.4309\n",
      "epoch: 700, loss = 0.3826\n",
      "epoch: 800, loss = 0.3657\n",
      "epoch: 900, loss = 0.3431\n",
      "epoch: 1000, loss = 0.3354\n",
      "epoch: 1100, loss = 0.3141\n",
      "epoch: 1200, loss = 0.3026\n",
      "epoch: 1300, loss = 0.2908\n",
      "epoch: 1400, loss = 0.2806\n",
      "epoch: 1500, loss = 0.2788\n",
      "epoch: 1600, loss = 0.2652\n",
      "epoch: 1700, loss = 0.2548\n",
      "epoch: 1800, loss = 0.2516\n",
      "epoch: 1900, loss = 0.2391\n",
      "epoch: 2000, loss = 0.2342\n"
     ]
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "for epoch in range(2000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred  = model(X_train)\n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "\n",
    "# 120: 0.4216\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9572\n"
     ]
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    logits = model(X_test)\n",
    "    y_pred = nn.Softmax(dim=1)(logits)\n",
    "    y_predicted_cls = y_pred.argmax(1)\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy: {acc.item():.4f}')\n",
    "\n",
    "# 0.9154\n",
    "# 0.9572\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       980\n",
      "           1       0.98      0.99      0.98      1135\n",
      "           2       0.95      0.96      0.96      1032\n",
      "           3       0.96      0.96      0.96      1010\n",
      "           4       0.96      0.96      0.96       982\n",
      "           5       0.95      0.95      0.95       892\n",
      "           6       0.97      0.96      0.97       958\n",
      "           7       0.96      0.94      0.95      1028\n",
      "           8       0.93      0.94      0.93       974\n",
      "           9       0.94      0.93      0.94      1009\n",
      "\n",
      "    accuracy                           0.96     10000\n",
      "   macro avg       0.96      0.96      0.96     10000\n",
      "weighted avg       0.96      0.96      0.96     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_predicted_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3  Questions (10 points )\n",
    "1.What's the difference between logistic regression and Perceptron?\n",
    "\n",
    "Perception is nonlinear while logistic regression is linear. MLP uses CrossEntropy as its loss function. \n",
    "\n",
    "\n",
    "2.Advantages and disadvantages of neural networks?\n",
    "\n",
    "Advantages: 1. There are more abundant data features. 2. It can fit nonlinear data.\n",
    "\n",
    "Disadvantages: 1. it is very complex. 2. It is hard to adjust the structures and parameters. 3. Gradient explosion & gradient disappearance. 4. Often cost more compute resources. \n",
    "\n",
    "3.What is the role of Activation Function in Neural networks?\n",
    "\n",
    "Limit the range of original data, change the original relationship. Change the linear relationship to fit the nonlinear mapping of the model. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
